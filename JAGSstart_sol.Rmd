---
title: "Exercise 4: Introduction to BUGS & JAGS"
output:
  html_document:
    toc: false
    number_sections: false
---


```{r, include=FALSE, }
knitr::opts_chunk$set(tidy = TRUE, message = FALSE, cache=FALSE)

```

The [BUGS project](https://www.mrc-bsu.cam.ac.uk/software/bugs/) (*Bayesian inference Using Gibbs Sampling*) was initiated in 1989 by the MRC (*Medical Research Council*) Biostatistical Unit at the University of Cambridge (United-Kingdom) to develop a flexible and user-friendly software for Bayesian analysis of complex models through MCMC algorithms. Its most famous and original implementation is `WinBUGS`, a clicking software available under *Windows*. `OpenBUGS` is an alternative implementation of `WinBUGS` running on either *Windows*, *Mac OS* ou *Linux*. [`JAGS`](http://mcmc-jags.sourceforge.net/) (*Just another Gibbs Sampler*) is a different and newer implementation that also relies on the `BUGS` language. Finally, the [`STAN`](http://mc-stan.org/) software must also be mentionned, recently developed et the Columbia Univeristy, ressemble `BUGS` through its interface, but relies on innovative MCMC approaches, such as Hamiltonian Monte Carlo, or variational Bayes approaches. A very useful resource is the [JAGS user manual](http://sourceforge.net/projects/mcmc-jags/files/Manuals/3.x/jags_user_manual.pdf).

To familiarise yourself with `JAGS` (and its `R` interface through the package `rjags`), we will look here at the *posterior* estimation of the mean and variance of observed data that we will model with a Gaussian distribution.

  0. Start by loading the `R` package `rjags`.
  
      ```{r lib1, eval = TRUE, echo = TRUE}
      library(rjags)
      ```

A `BUGS` model has 3 components: 
  
  - *the model*: specified in an external text file (`.txt`) according to a specific `BUGS` syntax
  - *the data*: a list containing each observation under a name matching the one used in the model specification
  - *the initial values*: (optional) a list containing the initial values for the various parameters to be estimated

  1. Sample $N=50$ observations from a Gaussian distribution with mean $m = 2$ and standard deviation $s = 3$ using the `R` function `rnorm` and store it into an object called `obs`.
  
      ```{r, eval = TRUE, echo = TRUE}
      N <- 50 # the number of observations
      obs <- rnorm(n = N, mean = 2, sd = 3) # the (fake) observed data
      ```
  
  2. Read the help of the `rjags` package, then save a text file (`.txt`) the following code defining the `BUGS` model: 
      ```{BUGS normal-model, echo=TRUE}
      # Model
      model{
        
        # Likelihood
        for (i in 1:N){ 
          obs[i]~dnorm(mu,tau)
        }
        
        # Prior
        mu~dnorm(0,0.0001) # proper but very flat (so weakly informative)
        tau~dgamma(0.0001,0.0001) # proper, and weakly informative (conjugate for Gaussian)
        
        # Variables of interest
        sigma <- pow(tau, -0.5)
      }
      ```

Each model specification file must start with the instruction `model{` indicating `JAGS` it is about to recieve a model specification. Then the model must be set up, usually by cycling along the data with a `for` loop. Here, we want to declare `N` observations, and each of them `obs[i]` follows a Gaussian distribution (characterized with the command `dnorm`) of mean `mu` and precision `tau`.  
***Warning***: in `BUGS`, the Gaussian distribution is parameterized by its **precision**, which is simply the inverse of the variance ($\tau = 1/\sigma^2$). Then, one needs to define the *prior* distribution for each parameter \--- here both `mu` and `tau`. For `mu`, we use a Gaussian *prior* with mean $0$ and precision $10^{-4}$ (thus variance $10,000$: this corresponds to a weakly informative *prior* quite spread out given the scale of our data. For `tau` we use the conjugate *prior* for precision in a Gaussian model, namely the Gamma distribution (with very small parameters, here again to remain the least informative possible). Finally, we give a deterministic definition of the additional parameters of interest, here the standard deviation `sigma`.  
**NB**: `~` indicates probabilistic distribution definition of a random variable, while `<-` indicates a deterministic calculus definition.
  
  3. With the `R` function `jags.model()`, create a `jags` object `R`.
        ```{r 1e-jags, eval = TRUE, echo = TRUE}
        myfirstjags <- jags.model("normalBUGSmodel.txt", data = list('obs' = obs, 
                                                                        'N' = length(obs)))
        ```
        
  4. With the `R` function `coda.samples()`, generate a sample of size $2,000$ from the *posterior* distributions for the mean and standard deviation parameters.
        ```{r update-1er-jags, eval = TRUE, echo = TRUE}
        res <- coda.samples(model = myfirstjags, variable.names = c('mu', 'sigma'), 
                            n.iter = 2000)
        ```
        
  5. Study the output of the `coda.samples()` `R` function, and compute both the *posterior* mean and median estimates for `mu` and `sigma`. Give a credibility interval at 95% for both.
        ```{r codasamples, eval = TRUE, echo = TRUE, out.width="67%", fig.height=6}
        plot(res)
        resum <- summary(res)
        resum
        resum$statistics["mu", "Mean"]
        resum$statistics["sigma", "Mean"]
        resum$quantiles["mu", "50%"]
        resum$quantiles["sigma", "50%"]
        resum$quantiles["mu", c(1,5)]
        resum$quantiles["sigma", c(1,5)]
        ```
        
  
  6. Load the `coda` `R` package. This package functions for convergence diagnostic and analysis of MCMC algorithm outputs.
      ```{r libcoda, eval = TRUE, echo = TRUE}
      library(coda)
      ```
  7. To diagnose the convergence of an MCMC algorithm, it is necessary to generate different Markov chains, with different initial values. Recreate a new `jags` object in `R` and specify 3 the use of 3 Markov chains with the argument `n.chains`, and initialize `mu` and `tau` at $0, -10, 100$ and at $1, 0.01, 0.1$ respectively with the argument `inits` (**ProTip:** use a `list` of `list`, one for each chain).  
        ```{r 2e-jags, eval = TRUE, echo = TRUE, fig.width = 12, fig.height = 8, out.width = "95%"}
        myjags2 <- jags.model("normalBUGSmodel.txt", data = list('obs' = obs, 'N' = N),
                               n.chains = 3, inits = list(list("mu" = 0, "tau" = 1), 
                               list("mu" = -10, "tau" = 1/100), 
                               list("mu" = 100, "tau" = 1/10)))
        res2 <- coda.samples(model = myjags2, variable.names = c('mu', 'sigma'), 
                             n.iter = 1000)
        plot(res2)
        ```
  8. With the `R` function `gelman.plot()`, plot the Gelman-Rubin statistic.
        ```{r gelman, eval = TRUE, echo = TRUE, out.width = "95%", fig.height = 4}
          gelman.plot(res2)
        ```
  9.  With the `R` functions `autocorr.plot()` and `acfplot()` evaluate the autocorrÃ©lation of the studied parameters.
        ```{r acf, eval = TRUE, echo = TRUE, out.height="20%"}
        acfplot(res2)
        ```
        ```{r autocorr, eval = TRUE, echo = TRUE, fig.height=8}
        par(mfrow=c(3,2))
        autocorr.plot(res2, ask = FALSE, auto.layout = FALSE)
        ```
        
  10.  With the `R` function `cumuplot()` evaluate the running quantiles of the studied parameters. How can you interpret them ?
        ```{r cumuplot, eval = TRUE, echo = TRUE, fig.height=8}
        par(mfrow=c(3,2))
        cumuplot(res2, ask=FALSE, auto.layout = FALSE)
        ```
        ```{r cumplot expl, eval = TRUE, echo = FALSE, results='asis'}
        cat("*Each row of the above graph is a different chain. The cumulative quantiles are indeed stable after the first few iterations in all chains.*")
        ```

 11. With the `R` function `crosscorr.plot()` evaluate the correlations between the studied parameters. How can you interpret them ?
        ```{r cumplot, eval = TRUE, echo = TRUE}
        crosscorr.plot(res2)
        ```
  12. With the function `hdi()` from the `R` package `HDInterval`, provide highest densitity *posterior* credibility intervals at 95%, and compare them to those obtained with the $2.5$% and $97.5$% quantiles.
        ```{r HDI, eval = TRUE, echo = TRUE}
        hdCI <- HDInterval::hdi(res2)
        hdCI
        symCI <- summary(res2)$quantiles[ ,c(1,5)]
        symCI
        
        symCI[, 2] - symCI[, 1]
        hdCI[2, ] - hdCI[1, ]
        ```
 
 